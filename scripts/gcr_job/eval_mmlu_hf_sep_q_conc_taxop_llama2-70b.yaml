description: test llama2-70b on MMLU


# target:
#   service: aml
#   name: alta2

# target:
#   service: sing
#   name: msroctovc
#   resource_group: gcr-singularity-octo
#   # workspace_name: Workspace_NLC
#   workspace_name: NLC_Workspace

# target:
#   service: sing
#   name: msrresrchvc
#   resource_group: gcr-singularity-resrch
#   workspace_name: Workspace_NLC

# target:
#   service: sing
#   name: msroctobasicvc
#   resource_group: gcr-singularity-octo
#   # workspace_name: Workspace_NLC
#   workspace_name: NLC_Workspace

target:
  service: sing
  name: msrresrchbasicvc
  resource_group: gcr-singularity
  # workspace_name: Workspace_NLC
  workspace_name: NLC_Workspace


# environment:
#   image: nvcr:v23.10
#   registry: shumingdocker.azurecr.io
#   setup:
#   - echo "master_addr:" "$$MASTER_ADDR"
#   - echo "master_port:" $$MASTER_PORT
#   - echo "node_rank:" $$OMPI_COMM_WORLD_RANK
#   username: shumingdocker

environment:
  # image: hangbo/pytorch-2.23dev:xformers
  # image: hangbo/pytorch-2.23dev:xformers_s2
  image: amlt-sing/acpt-2.3.1-py3.10-cuda12.1
  setup:
  - echo "master_addr:" "$$MASTER_ADDR"
  - echo "master_port:" $$MASTER_PORT
  - echo "node_rank:" $$OMPI_COMM_WORLD_RANK


code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/..


storage:
  qilongma:
    storage_account_name: msranlpintern
    container_name: qilongma
#   msranlp:
#     storage_account_name: msranlp
#     container_name: unilm
#   nlcredstone:
#     storage_account_name: nlcredstone
#     container_name: unilm
#   conversationhub:
#     storage_account_name: conversationhub
#     container_name: unilm
#   conversationhubhot:
#     storage_account_name: conversationhubhot
#     container_name: tengchaolv


# list of jobs to run, we run 2 jobs in this example
# jobs:
# - name: high_lr
#   sku: G1
#   command:
#   - python main.py --lr 0.5
# - name: low_lr
#   sku: G1
#   command:
#   - python main.py --lr 0.1


search:
  job_template:
    name: test_hf_sep_q_conc_taxop_llama2-70b_1st_basicvc
    sku: 1x40G8
    # sku: 2x40G8-A100-IB-NvLink
    identity: managed
    sla_tier: Basic
    priority: High
    # mpi: True
    # process_count_per_node: 8
    command:
    # - pip install -e ".[torch,metrics]"
    # - pip install torch pandas transformers
    # - pip install deepspeed==0.14.4
    - pip install -r requirements.txt
    - echo $${script}
    # - bash amlt_job/sas_mount.sh
    # - pip install -U flash-attn==2.4.2 --no-build-isolation
    - export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    - export AMLT_OUTPUT_DIR=/mnt/qilongma/amlt_output/ql-job
    - ls /mnt/qilongma
    # - ls /mnt/msranlp
    # - ls /mnt/nlcredstone
    # - ls /mnt/conversationhub
    # - ls /mnt/conversationhubhot
    # - FORCE_TORCHRUN=1 llamafactory-cli train bash_script/{script}
    # - torchrun --nproc_per_node=8 --nnodes=2 --node_rank=$$OMPI_COMM_WORLD_RANK --master_addr="$$MASTER_ADDR" --master_port=$$MASTER_PORT src/train.py bash_script/{script}
    # - torchrun --nproc_per_node=8 src/train.py bash_script/{script}
    - python -u evaluate_hf_feynman_sep_question_concept_taxo_path.py
      --data_dir /mnt/qilongma/mmlu_data
      --expl_dir /mnt/qilongma/explanations
      --save_dir /mnt/qilongma/results
      --model_path /mnt/qilongma/public_models/Meta-Llama-2-70B-hf
      --model_name Meta-Llama-2-70B
      --taxo_path_src gen
      --expl_model_name OpenAI-GPT-4o-mini
    submit_args:
      env:
        {"SINGULARITY_MPI_ENV":"-mca pml ucx --mca btl ^vader,tcp,openib -x NCCL_SOCKET_IFNAME=bond0 -x NCCL_IB_HCA=mlx5_0,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_9,mlx5_10,mlx5_11 -x NCCL_DEBUG=INFO",
        "SHARED_MEMORY_PERCENT":0.5}
    # tags: [Project_Name:1.58-bit-LLMs, ProjectID:PRJ-0349-A54, Experiment:BitNet-scaling]

  type: grid
  max_trials: 500
  params:
    - name: script
      spec: discrete
      # values: ['ta_chosen_llama3.1_instruct_dpo_2048_default_template_job.yaml', 'ta_chosen_tuluv2_dpo_2048_default_template_job.yaml', 'ta_rejected_llama3.1_instruct_dpo_2048_default_template_job.yaml', 'ta_rejected_tuluv2_dpo_2048_default_template_job.yaml']
      # values: ['ta_rejected_tuluv2_dpo_2048_default_template_job.yaml', 'ta_rejected_tuluv2_dpo_2048_default_template_v2_job.yaml', 'ta_rejected_tuluv2_dpo_2048_default_template_v3_job.yaml']
      # values: ['glanchatv2_full_sft_2048_default_template_job_lr5e6_e3.yaml', 'glanv2_full_sft_2048_default_template_job_lr5e6_e3.yaml', 'glanv2_glanchatv2_full_sft_2048_default_template_job_lr5e6_e3.yaml']
      # values: ['glanv2_glanchatv2_full_sft_2048_default_template_job_lr5e6_e3.yaml']
      values: ['']

